import streamlit as st
from dotenv import load_dotenv
import os

from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

from ingest import processar_documentos

load_dotenv()

st.title("Assistente Interno da Claro - Prot√≥tipo")

# --- EXPLICA√á√ÉO DA TECNOLOGIA ---
st.markdown("""
### üöÄ O que √© essa tecnologia?

Criamos um **Assistente Inteligente baseado em RAG (Retrieval-Augmented Generation)** ‚Äî uma tecnologia que permite que a IA responda de forma precisa usando exclusivamente documentos internos da empresa.

Ela funciona assim:

- üìÑ **L√™ documentos internos** como PDFs, pol√≠ticas, manuais e materiais de onboarding.  
- üîç **Transforma o conte√∫do em vetores** por meio de embeddings (FAISS).  
- ‚ùì Quando uma pergunta √© realizada:
  - O sistema busca automaticamente os trechos mais relevantes nos documentos.
  - Esses trechos s√£o enviados como contexto para o modelo de IA.
- üß† A IA gera uma resposta clara baseada apenas no conte√∫do dispon√≠vel nos documentos da empresa.

Esse processo garante respostas r√°pidas, consistentes e alinhadas com as informa√ß√µes institucionais.
""")

st.write("Pergunte sobre RH, TI ou documentos internos.")

# ---- BOT√ÉO DE INGEST ----
if st.button("üîÑ Atualizar base vetorial (rodar ingest)"):
    with st.spinner("Processando documentos e criando base vetorial..."):
        processar_documentos()
    st.success("Base vetorial criada com sucesso!")

# ---- VERIFICAR SE A BASE EXISTE ----
if not os.path.exists("base_faiss"):
    st.warning("‚è≥ Base vetorial n√£o encontrada. Clique no bot√£o acima para rodar o ingest.")
    st.stop()

# ---- CARREGAR A BASE ----
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.load_local("base_faiss", embeddings, allow_dangerous_deserialization=True)

# ---- OPENAI ----
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ---- CAMPO DE PERGUNTA ----
pergunta = st.text_input("Digite sua pergunta:")

if pergunta:

    resultados = db.similarity_search(pergunta, k=5)

    if len(resultados) == 0:
        st.error("‚ùå N√£o encontrei essa informa√ß√£o nos documentos internos.")
        st.stop()

    contexto = "\n\n".join([doc.page_content for doc in resultados])

    prompt = f"""
    Responda APENAS com base nos documentos abaixo:

    {contexto}

    Pergunta: {pergunta}

    Se n√£o houver resposta nos documentos, diga que n√£o consta.
    """

    resposta = llm.invoke(prompt)
    st.write("### Resposta")
    st.write(resposta.content)

    with st.expander("Documentos utilizados"):
        for doc in resultados:
            st.write(doc.page_content[:500] + "...")
